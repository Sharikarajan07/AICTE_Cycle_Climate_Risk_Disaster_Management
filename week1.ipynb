# Week 1 - Data Preprocessing & EDA for CSV datasets
# AICTE Cycle 3 (2025) - Climate Risk & Disaster Management

# ==============================
# ğŸ“Œ Import Libraries
# ==============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils.class_weight import compute_class_weight
import warnings
warnings.filterwarnings("ignore")

# ==============================
# ğŸ“Œ Load Dataset
# ==============================
DATA_PATH = "./Climate_Data/sample.csv"   # change to your file
df = pd.read_csv(DATA_PATH)

print("âœ… CSV dataset loaded with shape:", df.shape)
print(df.head())

# ==============================
# ğŸ“Œ Data Cleaning
# ==============================
print("\nMissing Values Before:\n", df.isnull().sum())

# Option 1: Drop rows with missing values
# df = df.dropna()

# Option 2: Fill missing values (better for time-series)
df = df.fillna(method="ffill").fillna(method="bfill")

print("âœ… Missing values handled. After:\n", df.isnull().sum())

# Encode categorical columns (e.g., 'Region', 'DisasterType')
for col in df.select_dtypes(include=["object"]).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    print(f"ğŸ”‘ Encoded column: {col}")

# ==============================
# ğŸ“Œ Exploratory Data Analysis (EDA)
# ==============================
print("\nğŸ“Š Basic Statistics:\n", df.describe())

# Correlation heatmap
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), annot=False, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# Distribution of target column (if exists)
if "label" in df.columns:
    plt.figure(figsize=(6,4))
    sns.countplot(x="label", data=df)
    plt.title("Target Distribution")
    plt.show()

# ==============================
# ğŸ“Œ Train-Test Split
# ==============================
if "label" in df.columns:
    X = df.drop("label", axis=1)
    y = df["label"]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print("âœ… Train-Test Split:")
    print("   Train:", X_train.shape, " Test:", X_test.shape)

    # Compute class weights (important for imbalanced disaster datasets)
    class_weights = compute_class_weight("balanced", classes=np.unique(y_train), y=y_train)
    print("\nâš–ï¸ Class Weights:", dict(zip(np.unique(y_train), class_weights)))

else:
    print("â„¹ï¸ No 'label' column found. Skipping train-test split.")
